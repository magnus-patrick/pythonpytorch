{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62d9719",
   "metadata": {},
   "source": [
    "# PyTorch Library (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0557a",
   "metadata": {},
   "source": [
    "Import these libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cae4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6e664",
   "metadata": {},
   "source": [
    "# NumPy Arrays and PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79861876",
   "metadata": {},
   "source": [
    "Tensors are VERY fundamental to deep learning models or neural networks in PyTorch. In many ways, they are very similar to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11784b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0, 10, 50)\n",
    "t = torch.linspace(0, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b76b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.2041,  0.4082,  0.6122,  0.8163,  1.0204,  1.2245,  1.4286,\n",
       "         1.6327,  1.8367,  2.0408,  2.2449,  2.4490,  2.6531,  2.8571,  3.0612,\n",
       "         3.2653,  3.4694,  3.6735,  3.8776,  4.0816,  4.2857,  4.4898,  4.6939,\n",
       "         4.8980,  5.1020,  5.3061,  5.5102,  5.7143,  5.9184,  6.1224,  6.3265,\n",
       "         6.5306,  6.7347,  6.9388,  7.1429,  7.3469,  7.5510,  7.7551,  7.9592,\n",
       "         8.1633,  8.3673,  8.5714,  8.7755,  8.9796,  9.1837,  9.3878,  9.5918,\n",
       "         9.7959, 10.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed4392a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.20408163,  0.40816327,  0.6122449 ,  0.81632653,\n",
       "        1.02040816,  1.2244898 ,  1.42857143,  1.63265306,  1.83673469,\n",
       "        2.04081633,  2.24489796,  2.44897959,  2.65306122,  2.85714286,\n",
       "        3.06122449,  3.26530612,  3.46938776,  3.67346939,  3.87755102,\n",
       "        4.08163265,  4.28571429,  4.48979592,  4.69387755,  4.89795918,\n",
       "        5.10204082,  5.30612245,  5.51020408,  5.71428571,  5.91836735,\n",
       "        6.12244898,  6.32653061,  6.53061224,  6.73469388,  6.93877551,\n",
       "        7.14285714,  7.34693878,  7.55102041,  7.75510204,  7.95918367,\n",
       "        8.16326531,  8.36734694,  8.57142857,  8.7755102 ,  8.97959184,\n",
       "        9.18367347,  9.3877551 ,  9.59183673,  9.79591837, 10.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3803165a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.2041,  0.4082,  0.6122,  0.8163],\n",
       "         [ 1.0204,  1.2245,  1.4286,  1.6327,  1.8367],\n",
       "         [ 2.0408,  2.2449,  2.4490,  2.6531,  2.8571],\n",
       "         [ 3.0612,  3.2653,  3.4694,  3.6735,  3.8776],\n",
       "         [ 4.0816,  4.2857,  4.4898,  4.6939,  4.8980]],\n",
       "\n",
       "        [[ 5.1020,  5.3061,  5.5102,  5.7143,  5.9184],\n",
       "         [ 6.1224,  6.3265,  6.5306,  6.7347,  6.9388],\n",
       "         [ 7.1429,  7.3469,  7.5510,  7.7551,  7.9592],\n",
       "         [ 8.1633,  8.3673,  8.5714,  8.7755,  8.9796],\n",
       "         [ 9.1837,  9.3878,  9.5918,  9.7959, 10.0000]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(2, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f2e1503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.20408163,  0.40816327,  0.6122449 ,\n",
       "          0.81632653],\n",
       "        [ 1.02040816,  1.2244898 ,  1.42857143,  1.63265306,\n",
       "          1.83673469],\n",
       "        [ 2.04081633,  2.24489796,  2.44897959,  2.65306122,\n",
       "          2.85714286],\n",
       "        [ 3.06122449,  3.26530612,  3.46938776,  3.67346939,\n",
       "          3.87755102],\n",
       "        [ 4.08163265,  4.28571429,  4.48979592,  4.69387755,\n",
       "          4.89795918]],\n",
       "\n",
       "       [[ 5.10204082,  5.30612245,  5.51020408,  5.71428571,\n",
       "          5.91836735],\n",
       "        [ 6.12244898,  6.32653061,  6.53061224,  6.73469388,\n",
       "          6.93877551],\n",
       "        [ 7.14285714,  7.34693878,  7.55102041,  7.75510204,\n",
       "          7.95918367],\n",
       "        [ 8.16326531,  8.36734694,  8.57142857,  8.7755102 ,\n",
       "          8.97959184],\n",
       "        [ 9.18367347,  9.3877551 ,  9.59183673,  9.79591837,\n",
       "         10.        ]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.reshape(2, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e29a53",
   "metadata": {},
   "source": [
    "## General Broadcasting Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499076f",
   "metadata": {},
   "source": [
    "NumPy compares the shapes of arrays when doing operations on them, starting with the right-most dimensions and moving left. Two dimensions are compatible when they're equal or when one of them is 1.\n",
    "\n",
    "For example, an array with this shape: (1, 2, 3, 4)\n",
    "is compatible with an array with this shape: (5, 6, 7, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0494bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((6, 5))\n",
    "b = np.arange(5).reshape((1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61133352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4efcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f62cc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30fd286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((6, 5))\n",
    "b = torch.arange(5).reshape((1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25bc48b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.],\n",
       "        [0., 1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488dc11",
   "metadata": {},
   "source": [
    "### 1 Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa096f",
   "metadata": {},
   "source": [
    "The arrays/tensors don't need to have the same # of dimensions for broadcasting.\n",
    "\n",
    "Example: Scaling the color channels of an image by a different amount.\n",
    "\n",
    "```python\n",
    "Image (3D Array): 256 x 256 x 3\n",
    "\n",
    "Scale (1D Array):             3\n",
    "\n",
    "Result (3D Array): 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e13df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((256, 256, 3)) # 256 height, 256 width, 3 color channels.\n",
    "scale = torch.tensor((0.5, 1.5, 1))\n",
    "result = image*scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e5215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0525e+00, -1.6660e+00,  3.9438e-01],\n",
       "         [ 1.3701e+00,  8.0176e-01,  2.7599e-01],\n",
       "         [ 4.1844e-01,  1.3582e+00,  3.1026e-01],\n",
       "         ...,\n",
       "         [-1.0780e+00,  1.2455e+00, -3.3096e-01],\n",
       "         [ 1.0358e+00,  1.1786e+00,  1.2781e+00],\n",
       "         [ 9.6587e-03,  1.8907e+00, -1.1445e+00]],\n",
       "\n",
       "        [[ 8.2376e-01, -1.2966e+00, -2.9366e-01],\n",
       "         [-9.5605e-01,  1.5134e+00, -4.1749e-01],\n",
       "         [ 2.7789e-01,  3.5184e-01, -2.6870e-03],\n",
       "         ...,\n",
       "         [ 5.7974e-01, -7.9050e-01,  2.8802e+00],\n",
       "         [-7.0645e-02,  6.2995e-01,  9.0971e-01],\n",
       "         [ 1.5242e+00, -6.8651e-01, -6.5262e-01]],\n",
       "\n",
       "        [[ 1.9966e-01,  3.2253e-01,  2.0364e-01],\n",
       "         [-3.0004e-02,  4.6945e-01, -6.5543e-01],\n",
       "         [ 2.5728e-01, -3.0045e-01,  9.3284e-01],\n",
       "         ...,\n",
       "         [-4.0095e-01,  3.7103e-01, -1.2680e+00],\n",
       "         [ 9.1475e-01,  1.0439e-01, -1.1703e+00],\n",
       "         [-1.4955e+00, -2.2228e+00, -4.8915e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.4441e-01,  1.4840e-01, -6.7886e-01],\n",
       "         [ 8.7417e-01, -5.8596e-01,  6.0173e-01],\n",
       "         [-7.2643e-01, -1.4554e+00, -8.5483e-01],\n",
       "         ...,\n",
       "         [-9.1327e-01, -4.1173e-01, -3.7411e-01],\n",
       "         [ 5.6857e-01,  1.1124e+00,  5.4677e-01],\n",
       "         [-7.1350e-02,  1.8402e+00,  1.4106e-01]],\n",
       "\n",
       "        [[ 5.3727e-01,  5.2036e-01, -1.5224e-01],\n",
       "         [ 7.7779e-01,  1.0663e+00,  2.8310e-01],\n",
       "         [ 4.3303e-01,  7.6060e-01,  8.0893e-01],\n",
       "         ...,\n",
       "         [ 9.0131e-01,  8.7278e-01,  6.1519e-01],\n",
       "         [ 6.2119e-01, -2.0046e+00, -4.9148e-01],\n",
       "         [-6.3613e-02, -7.3181e-01,  2.9343e-01]],\n",
       "\n",
       "        [[-6.7018e-01,  1.1828e+00,  9.4132e-01],\n",
       "         [ 4.8581e-01, -3.8406e-01, -5.1291e-01],\n",
       "         [-4.0474e-01, -1.2410e-01,  4.0754e-01],\n",
       "         ...,\n",
       "         [ 8.2310e-01, -1.2367e+00, -1.2021e+00],\n",
       "         [-1.2055e+00, -2.3679e+00,  4.7406e-01],\n",
       "         [ 1.5859e+00,  1.5045e+00, -1.0509e+00]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9802bb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0798,  1.7369,  0.6174],\n",
       "         [-0.1516,  0.4027, -2.5925],\n",
       "         [ 0.4811, -2.3056, -0.3415],\n",
       "         ...,\n",
       "         [-0.1669, -0.9883,  1.0752],\n",
       "         [ 0.2563, -1.5196, -0.1932],\n",
       "         [ 0.0388, -1.1467, -1.0058]],\n",
       "\n",
       "        [[ 0.2866,  0.8284,  0.2383],\n",
       "         [-0.3464, -1.2435, -0.6764],\n",
       "         [-0.1164, -0.3575,  0.4215],\n",
       "         ...,\n",
       "         [ 0.7877, -1.1866,  0.6854],\n",
       "         [ 0.2535, -1.5809,  1.9379],\n",
       "         [ 0.2320, -0.9515, -0.7418]],\n",
       "\n",
       "        [[ 0.0129, -1.8592,  0.9526],\n",
       "         [-0.2234, -0.7306, -0.8008],\n",
       "         [-0.7287,  2.3110, -0.7538],\n",
       "         ...,\n",
       "         [ 0.3423,  1.4453,  0.0218],\n",
       "         [ 0.3247,  1.7774,  2.0419],\n",
       "         [ 0.0248,  1.3605, -0.1211]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3353, -0.0319,  0.4101],\n",
       "         [-0.4515, -1.2659, -0.2063],\n",
       "         [-0.2784, -1.1100, -1.5977],\n",
       "         ...,\n",
       "         [-0.5511,  1.2321, -0.5514],\n",
       "         [ 0.2646, -1.2557,  2.9311],\n",
       "         [ 0.2170,  2.5884, -0.5897]],\n",
       "\n",
       "        [[ 0.4872,  3.0976,  0.5120],\n",
       "         [ 0.0258,  2.5094, -1.8066],\n",
       "         [ 0.2174,  0.7030, -0.5437],\n",
       "         ...,\n",
       "         [-0.1218,  1.2992, -0.3304],\n",
       "         [ 0.0181, -0.4449,  0.4493],\n",
       "         [ 0.3449, -2.4278, -0.2525]],\n",
       "\n",
       "        [[-0.4673,  0.8046, -1.3431],\n",
       "         [-0.3295, -0.5093,  0.0407],\n",
       "         [ 0.2113, -0.2011, -0.8128],\n",
       "         ...,\n",
       "         [-0.4112,  0.8163,  2.7066],\n",
       "         [-0.1800,  0.4864, -0.0281],\n",
       "         [-0.7606,  1.8682, -0.0687]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8e956",
   "metadata": {},
   "source": [
    "### 2 Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0001b2",
   "metadata": {},
   "source": [
    "The arrays/tensors don't need to have the same # of dimensions for broadcasting.\n",
    "\n",
    "Example: Array of 2 images and wanting to scale the color channels of each image by a slightly different amount.\n",
    "\n",
    "```python\n",
    "Images (4D Array): 2 x 256 x 256 x 3\n",
    "\n",
    "Scales (4D Array): 2 x 1 x 1 x 3\n",
    "\n",
    "Results (4D Array): 2 x 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77c1aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.randn((2, 256, 256, 3)) # First number (2) represents the batch size of 2 images. 256 pixels in height, 256 pixels in width, and 3 color channels for RGB.\n",
    "scales = torch.tensor([0.5, 1.5, 1, 1.5, 1, 0.5]).reshape((2, 1, 1, 3)) # 1x1 in reshape() allow the entire image to be broadcast to match the full image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f533ad2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8522,  1.0923,  0.7599],\n",
       "          [-1.0957, -2.8983,  0.1554],\n",
       "          [-0.5626, -0.6021,  0.6851],\n",
       "          ...,\n",
       "          [ 0.2110, -0.5126, -0.6695],\n",
       "          [-0.5228,  2.5122,  0.0654],\n",
       "          [ 0.2978,  0.6578, -0.1609]],\n",
       "\n",
       "         [[ 0.5921, -1.1434, -0.8861],\n",
       "          [ 0.2508,  2.9169, -0.9600],\n",
       "          [ 0.3760, -0.2648, -0.6563],\n",
       "          ...,\n",
       "          [ 1.2775, -0.7250, -1.7044],\n",
       "          [ 0.1054, -0.1606, -2.1434],\n",
       "          [ 0.8957,  2.0595, -0.6691]],\n",
       "\n",
       "         [[-0.1917, -1.0970,  0.9026],\n",
       "          [ 0.5043, -1.9590, -1.7855],\n",
       "          [-0.3218,  1.4676,  0.6270],\n",
       "          ...,\n",
       "          [-0.4307,  0.4541, -0.6349],\n",
       "          [-0.3854, -0.3487, -1.4607],\n",
       "          [ 0.2175, -0.4314,  1.8270]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.6955,  0.8256,  0.9564],\n",
       "          [-0.0899, -3.1959,  0.0832],\n",
       "          [ 0.2634, -0.1220,  0.3382],\n",
       "          ...,\n",
       "          [ 0.5373,  0.1287,  0.9361],\n",
       "          [ 0.3760,  2.8989,  2.2514],\n",
       "          [-0.9556,  0.1984, -0.9796]],\n",
       "\n",
       "         [[-0.1430, -1.6886,  0.4732],\n",
       "          [ 0.7628, -0.8398, -0.3294],\n",
       "          [ 0.1735,  2.7581, -0.6212],\n",
       "          ...,\n",
       "          [ 1.1137, -0.3378, -0.6101],\n",
       "          [-0.1706,  1.0055,  1.7197],\n",
       "          [ 0.1958, -1.2447, -0.7584]],\n",
       "\n",
       "         [[ 0.7057,  1.8428,  1.5056],\n",
       "          [-1.3058, -2.7590,  0.8264],\n",
       "          [ 0.3164, -1.6006, -0.7120],\n",
       "          ...,\n",
       "          [-0.1776,  1.3111,  0.7839],\n",
       "          [ 0.2512,  1.6314,  0.7904],\n",
       "          [ 0.4222,  2.2697, -0.7760]]],\n",
       "\n",
       "\n",
       "        [[[ 2.4586, -0.7099,  0.0848],\n",
       "          [-0.5523,  0.7051, -0.4374],\n",
       "          [-0.9626,  0.0835,  0.1137],\n",
       "          ...,\n",
       "          [-0.4392,  1.4230,  0.0101],\n",
       "          [-0.3399, -0.1997,  1.4661],\n",
       "          [ 0.7380,  0.1279, -0.1022]],\n",
       "\n",
       "         [[ 2.4293, -0.4422,  0.4421],\n",
       "          [-0.4249,  1.3828,  0.1464],\n",
       "          [ 0.7372,  1.3635, -0.0514],\n",
       "          ...,\n",
       "          [-1.0769,  1.3476, -0.4427],\n",
       "          [ 0.5745,  0.0830, -0.1337],\n",
       "          [-0.4950, -0.0659,  0.0700]],\n",
       "\n",
       "         [[-3.7469, -1.2315,  1.0989],\n",
       "          [-1.6039, -0.0131, -0.1473],\n",
       "          [ 1.5703, -0.1882,  0.2234],\n",
       "          ...,\n",
       "          [ 1.0460,  1.3521,  0.1830],\n",
       "          [-1.5368, -0.9433, -0.4953],\n",
       "          [ 2.0527, -0.7464, -0.2039]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.6833, -1.3805,  0.9843],\n",
       "          [ 0.3756,  0.2955,  0.5470],\n",
       "          [-0.1117, -1.5169, -0.1717],\n",
       "          ...,\n",
       "          [-1.5689,  0.6813,  0.3729],\n",
       "          [ 1.9305,  1.0897, -0.0749],\n",
       "          [ 0.1792, -0.7033, -0.2022]],\n",
       "\n",
       "         [[ 3.1489, -0.0978,  0.1257],\n",
       "          [-1.3427, -0.3353,  0.3836],\n",
       "          [-0.1585,  1.4183,  0.1418],\n",
       "          ...,\n",
       "          [-1.2594, -0.0312, -0.4099],\n",
       "          [-0.5910, -0.7858,  0.7394],\n",
       "          [ 0.5208,  0.6163,  0.1658]],\n",
       "\n",
       "         [[ 2.0927,  0.2086, -0.1438],\n",
       "          [-1.6548, -1.1139, -0.2167],\n",
       "          [-0.5491, -1.3876, -1.1147],\n",
       "          ...,\n",
       "          [ 4.1910,  1.4204, -0.9025],\n",
       "          [-1.8080, -0.8483, -0.1870],\n",
       "          [ 1.0626,  2.1963,  0.5297]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = images * scales\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca2d2ce",
   "metadata": {},
   "source": [
    "# Operations Across Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043d09d",
   "metadata": {},
   "source": [
    "Very fundamental operations in PyTorch that are similar to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fb756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1250), tensor(1.6520), tensor(4.), tensor(0.5000))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([0.5, 1, 3, 4])\n",
    "torch.mean(t), torch.std(t), torch.max(t), torch.min(t) # Use .float() after the tensor if it consists of no floats as these functions often return decimals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1861a7",
   "metadata": {},
   "source": [
    "## 2D Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c34d9",
   "metadata": {},
   "source": [
    "For a 2D tensor, let's say we want to take the mean of the first column. Taking the mean of each column is the equivalent of taking the mean across the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac2e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(20, dtype = float).reshape(5, 4)\n",
    "torch.mean(t, axis = 0) # axis = 0 evaluates the rows while axis = 1 evaluates the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c64c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5fd881",
   "metadata": {},
   "source": [
    "This is possible for higher dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(4, 256, 256, 3) # 4 images, each 256x256 pixels with 3 color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b417a593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis = 0).shape # Mean across the batch of images when axis is 0; the first number in tensor t which is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f44db84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis = -1).shape # Takes mean of the color channels when axis is -1; the last number in the tensor which is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d31b5",
   "metadata": {},
   "source": [
    "Another operation is taking the maximum color channel values from each image. This has a variety of applications in image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93e58606",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.max(t, axis = -1) # Returns the brightest values for each channel along with their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "387a77d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 2, 0,  ..., 1, 0, 1],\n",
       "         [2, 1, 0,  ..., 1, 1, 0],\n",
       "         [2, 1, 0,  ..., 2, 1, 0],\n",
       "         ...,\n",
       "         [2, 2, 0,  ..., 1, 1, 2],\n",
       "         [2, 0, 2,  ..., 0, 1, 2],\n",
       "         [2, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 2, 0, 0],\n",
       "         [0, 2, 0,  ..., 2, 1, 1],\n",
       "         [2, 1, 0,  ..., 2, 2, 0],\n",
       "         ...,\n",
       "         [2, 0, 1,  ..., 2, 0, 1],\n",
       "         [2, 2, 0,  ..., 2, 2, 0],\n",
       "         [0, 2, 0,  ..., 2, 1, 2]],\n",
       "\n",
       "        [[1, 0, 2,  ..., 1, 1, 1],\n",
       "         [1, 2, 1,  ..., 1, 1, 1],\n",
       "         [1, 0, 2,  ..., 0, 2, 0],\n",
       "         ...,\n",
       "         [1, 2, 0,  ..., 2, 2, 2],\n",
       "         [0, 2, 1,  ..., 2, 0, 2],\n",
       "         [1, 1, 0,  ..., 2, 1, 0]],\n",
       "\n",
       "        [[2, 1, 1,  ..., 2, 0, 2],\n",
       "         [0, 2, 0,  ..., 1, 0, 1],\n",
       "         [1, 1, 1,  ..., 0, 1, 1],\n",
       "         ...,\n",
       "         [2, 0, 2,  ..., 0, 0, 1],\n",
       "         [1, 1, 2,  ..., 2, 2, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices # 0, 1, & 2 correspond to red, green, and blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e9200c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3262e+00,  7.6990e-01,  2.1199e+00,  ...,  1.2491e+00,\n",
       "         -2.3270e-01,  1.5231e+00],\n",
       "        [-1.0046e+00,  5.7784e-01,  1.0310e+00,  ...,  1.5387e+00,\n",
       "          9.0940e-01,  1.3376e+00],\n",
       "        [ 5.4676e-01,  1.6488e-03,  1.6760e+00,  ..., -2.4037e-02,\n",
       "          1.0200e+00,  5.1438e-01],\n",
       "        ...,\n",
       "        [ 7.4960e-01,  8.5924e-03,  1.6779e+00,  ..., -4.3419e-01,\n",
       "          7.9431e-01,  1.0055e+00],\n",
       "        [ 1.1981e+00,  3.0529e-01,  6.1339e-01,  ...,  2.9202e-01,\n",
       "          1.5079e+00,  1.0418e+00],\n",
       "        [ 4.3734e-01, -9.0478e-02,  1.3013e+00,  ...,  8.8235e-01,\n",
       "          1.7419e+00,  1.1982e+00]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef44a7",
   "metadata": {},
   "source": [
    "# Differences Between NumPy and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c5afa",
   "metadata": {},
   "source": [
    "They compute gradients of operations differently.\n",
    "\n",
    "$$y = \\sum_{i} x_i^3$$\n",
    "\n",
    "has a gradient of\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x_i} = 3x_i^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7665acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[3., 5.], [8., 7.]], requires_grad = True) # With the requires_grad parameter, PyTorch remembers the gradient when operations are done on the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c79e0985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1007., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.pow(3).sum() # .sum() is a special method for PyTorch as well as NumPy.\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f605e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.,  75.],\n",
       "        [192., 147.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() # Compute the gradient. Important for ML concepts such as gradient descent and backpropagation.\n",
    "x.grad # Print the gradient attribute of x for each element in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c483275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.,  75.],\n",
       "        [192., 147.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*x**2 # Check with the analytic derivative formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48072ba",
   "metadata": {},
   "source": [
    "Computing gradients is very vital to how neural networks learn from their mistakes. Neural networks can be described as functions. But, they are more convoluted than the above example so with them, you will rarely use analytic formulas for computing the gradients so PyTorch is used instead. \n",
    "\n",
    "After that, a neural network adjusts certain parameters called weights as well as biases in order to minimize what is called the loss function. The loss function measures errors in a network's predictions, and must therefore be minimized in order for the network to give more accurate outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5c137",
   "metadata": {},
   "source": [
    "# Additional Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b2f01",
   "metadata": {},
   "source": [
    "PyTorch does matrix multiplication with tensors much quicker than NumPy does with its arrays, and becomes even faster with the use of a GPU instead of a CPU. However, more memory is used up by tensors in order to do faster operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "63573a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04854950000299141"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn((1000, 1000))\n",
    "B = torch.randn((1000, 1000))\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "torch.matmul(A, B)\n",
    "t2 = time.perf_counter()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d417a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07111910000094213"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.randn(int(1e6)).reshape(1000, 1000)\n",
    "B = np.random.randn(int(1e6)).reshape(1000, 1000)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "A@B\n",
    "t2 = time.perf_counter()\n",
    "t2-t1 # Time is greater for NumPy so it is slower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
