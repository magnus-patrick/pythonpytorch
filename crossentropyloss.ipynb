{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1964ca49",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a2f04",
   "metadata": {},
   "source": [
    "Also known as logarithmic loss, it's a type of cost function like the MSE loss function. It calculates the entropy, or uncertainty, between the **true** probability distribution and the **predicted** probability distribution; how similar or different they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1e0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5cca7",
   "metadata": {},
   "source": [
    "# Initial Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29912a54",
   "metadata": {},
   "source": [
    "Suppose there's a set of numbers $\\vec {p} = [1, 3, 5, 2, ...]$ of length $N$. What set of corresponding numbers $\\vec {q} = [q_1, q_2, q_3, ...]$ minimizes the cross-entropy loss function defined as:\n",
    "\n",
    "$$H(\\vec{p}, \\vec{q}) = -\\sum_{i = 1}^{N} p_i \\ln(q_i)$$\n",
    "\n",
    "with a constraint $\\Sigma _i p_i= \\Sigma _i q_i$\n",
    "\n",
    "$p_i$ denotes the actual distribution of values; $q_i$ denotes the predicted distribution of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5d0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([5,4,3,2,1])\n",
    "q1 = p\n",
    "q2 = np.array([3,2,5,7,8])\n",
    "q3 = np.array([9,3,6,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729cb86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-23.528439171277483)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-sum(p*np.log(q3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a5885",
   "metadata": {},
   "source": [
    "Because this is a constrained optimization problem, Lagrange multipliers are useful. Defined as shown:\n",
    "\n",
    "$$f(\\vec{q}, \\lambda) = -\\sum_{i = 1}^{N} p_i \\ln(q_i) - \\lambda \\left(\\sum_{i}p_i - \\sum_{i} q_i \\right)$$\n",
    "\n",
    "Now, in order to find the extrema (for machine learning, it's the minimum) of the loss function, we set the partial derivative with respect to $q_i$ equal to 0. Same goes for the partial derivative with respect to $\\lambda$.\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial q_i} = -\\frac{p_i}{q_i} + \\lambda \\rightarrow= 0$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial \\lambda} = \\left(\\sum_{i} p_i - \\sum_{i} q_i \\right) \\rightarrow= 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f960402",
   "metadata": {},
   "source": [
    "From this, we get the 2 equations:\n",
    "\n",
    "- $p_i = \\lambda q_i \\rightarrow \\Sigma p_i = \\lambda \\Sigma q_i$\n",
    "- $\\Sigma p_i = \\Sigma q_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf5b709",
   "metadata": {},
   "source": [
    "These equations can only be true if $\\lambda = 1$ so we must have $q_i = p_i$, and thus $\\vec q = \\vec p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb25b2",
   "metadata": {},
   "source": [
    "# Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5eac2",
   "metadata": {},
   "source": [
    "In classification problems, an input is taken such as an image, called $x$. This image may contain an object like a ball, shoe, etc. We'll call the true probability of image $x$ belonging to class $i$ as $p_i$. The goal of a classifier is to create a function $f$ such that \n",
    "\n",
    "$$f(x) = \\vec q$$\n",
    "\n",
    "where $\\vec q$ is as close to $\\vec p$ as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1422a65",
   "metadata": {},
   "source": [
    "- Note that $\\vec p$ and $\\vec q$ are probability mass functions (PMFs), and each element of the vector represents different classes. Because probability is involved, it follows that $\\Sigma p_i = \\Sigma q_i = 1$. This is the constraint of the Lagrange multiplier above.\n",
    "\n",
    "- Typically, we know what class $\\tilde c$ an image $x$ belongs to. In this case, it's typically the case that $p_{\\tilde c} = 1$ for the class $i = \\tilde c$ that we know it is, and other $p_i$ s are equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d669a2",
   "metadata": {},
   "source": [
    "To minimize the difference between $\\vec p$ and $\\vec q$, we minimize the cross-entropy loss function:\n",
    "\n",
    "$$H(\\vec{p}, \\vec{q}) = -\\sum_{i} p_i \\ln(q_i)$$\n",
    "\n",
    "Since the minimum of this function occurs exactly when $p_i = q_i$ for all $i$. In this case, when we know which class an image belongs to (one of the $p_i$ s = 1), we have:\n",
    "\n",
    "$$H(\\vec{p}, \\vec{q}) = -\\ln(q_{\\tilde c})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1213da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.zeros(10); p[4] = 1\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52825400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20855977,  0.01010575, -0.00462332,  0.15392126,  0.06783992,\n",
       "       -0.09038419, -0.1227637 ,  0.2720416 ,  0.12023831,  0.38506459])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.random.randn(10)\n",
    "q = q/sum(q)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2fc0d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.69060445])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = -np.log(q[p > 0])\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2e97ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04555446])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[4] = 20\n",
    "q = q/sum(q)\n",
    "H = -np.log(q[p > 0])\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f204a6",
   "metadata": {},
   "source": [
    "# For Multiple Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec75b7",
   "metadata": {},
   "source": [
    "In this scenario, let's compute the loss over $N$ images ($x_n$). Suppose we know exactly what class the image belongs to. We can express the true class of the nth image as $\\tilde c (n)$. The probability of image $n$ belonging to class $c$ as $q_n (c)$.\n",
    "\n",
    "- Thus, the **predicted** probability of an image $n$ belonging to its true class $\\tilde c (n)$ is $q_n(\\tilde c (n))$.\n",
    "\n",
    "We sum it together:\n",
    "\n",
    "$$L(p, q) = \\sum_{n = 1}^{N}H({p_n}, {q_n}) = -\\sum_{n = 1}^{N} \\ln(q_n(\\tilde c (n)))$$\n",
    "\n",
    "Get a sample of $p$ s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8da2dc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.zeros((4, 10), dtype = int)\n",
    "p[0][4] = 1\n",
    "p[3][3] = 1\n",
    "p[2][8] = 1\n",
    "p[1][2] = 1\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc797e",
   "metadata": {},
   "source": [
    "Create probability array of $q$ s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9721cf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1531486 , -0.33646045,  0.17881717,  1.12984208,  0.01711937,\n",
       "         0.09428152, -0.10180105, -0.41069267,  0.0230275 ,  0.25271793],\n",
       "       [-0.13861425, -0.10440734,  0.2918362 ,  0.10760671,  0.3639906 ,\n",
       "         0.21897043,  0.66270117,  0.17414809, -0.35027397, -0.22595765],\n",
       "       [-0.0932851 ,  0.4827172 ,  0.30793316,  0.00392941, -0.08734484,\n",
       "        -0.02598965, -0.06607129,  0.0031672 ,  0.50359873, -0.02865483],\n",
       "       [ 0.54019793, -1.88490379,  0.25124035,  0.70971842,  0.42960954,\n",
       "        -0.08397891, -0.63278626,  1.06370509,  0.40170946,  0.20548817]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.random.randn(40).reshape(4, 10)\n",
    "q = q/np.expand_dims(np.sum(q, axis = 1), axis = 1) # axis = 1 within np.sum sums across rows.\n",
    "q # Probability of each of the 4 images belonging to one of 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f494f",
   "metadata": {},
   "source": [
    "Compute $H$ for every term in the sum. Then, sum together to calculate $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38181385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01711937, 0.2918362 , 0.50359873, 0.70971842])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[p>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55f2ac4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.06754454, 1.23156259, 0.68597551, 0.34288698])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hs = -np.log(q[p>0]) # Take natural log of every value of q where p is 1.\n",
    "Hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66f8a3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.327969625576402)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = sum(Hs) # Total loss L(p, q).\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299f691",
   "metadata": {},
   "source": [
    "# Obtaining the $q$ s in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2dbd9f",
   "metadata": {},
   "source": [
    "*$\\vec q$* should be related to a probability density function.\n",
    "\n",
    "- Bounded between 0 and 1 (Meaning that the data is normalized.)\n",
    "- The closer to 0, the less confidence we have that image $n$ belongs to class $c$\n",
    "- The closer to 1, the more confidence we have that image $n$ belongs to class $c$\n",
    "- $\\sum_{c=0}^{C} q_c = 1$ for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288fb10",
   "metadata": {},
   "source": [
    "Suppose a neural network outputs $f(x_n) = \\hat{y}_n$ where $\\hat{y}_n$ is a vector with the same length as the number of classes, but $\\hat{y}_n$ is not normalized like $\\vec q$ should be. We can enforce the last condition by normalizing the following way:\n",
    "\n",
    "$$q_n(c) = \\frac{\\exp \\left (\\hat{y}_n(c) \\right)}{\\sum_{c^{'} = 0}^{C} \\exp \\left (\\hat{y}_n(c^{'}) \\right)}$$\n",
    "\n",
    "So we can write the loss of the output vector as:\n",
    "\n",
    "$$L(\\hat{y}_n) = -\\sum_{n = 1}^{N} \\ln(q_n(\\tilde c (n))) = - \\sum_{n = 0}^{N} \\ln \\left(\\frac{\\exp \\left (\\hat{y}_n(\\tilde{c}(n)) \\right)}{\\sum_{c = 0}^{C} \\exp \\left (\\hat{y}_n(c) \\right)} \\right)$$\n",
    "\n",
    "- Note that for a given $n$, $\\hat{y}_n(c)$ is the network output for a given image $x_n$\n",
    "\n",
    "Get sample $\\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fef6979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00131002e+00, 1.98733066e+01, 5.96261095e+01, 1.09678714e+01,\n",
       "        2.46671490e+00, 9.13040898e-01, 4.12436157e-02, 7.83721322e+00,\n",
       "        7.48304803e+00, 9.69454301e-01],\n",
       "       [2.49066194e+00, 5.20570255e+01, 4.61273034e+01, 1.52378361e+01,\n",
       "        3.38444331e-01, 9.44825774e-01, 2.55229482e-01, 1.07898274e+01,\n",
       "        4.50163385e+01, 1.73982761e+01],\n",
       "       [4.34726323e+01, 2.64560271e-01, 1.38631843e+01, 7.49144768e+01,\n",
       "        6.65430380e+01, 3.61217176e+01, 2.07193740e+00, 1.21724136e+01,\n",
       "        1.50934946e+01, 4.90860942e+00],\n",
       "       [4.17048673e+01, 1.98287359e+01, 4.53426593e-01, 3.36044786e+00,\n",
       "        6.19356333e+01, 4.75274865e-02, 2.27418740e+01, 4.76039078e+00,\n",
       "        6.63599050e+00, 6.41687811e+01]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = 20*np.random.randn(40).reshape(4, 10) **2\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85ba88",
   "metadata": {},
   "source": [
    "First axis is with respect to $n$. The second is with respect to $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "874039bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d78aa5",
   "metadata": {},
   "source": [
    "Compute the $q$ s from the exponential fraction above (can be difficult):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e344c79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.55954081e-25, 5.43972662e-18, 1.00000000e+00, 7.37896904e-22,\n",
       "        1.49965151e-25, 3.17130712e-26, 1.32623826e-26, 3.22380031e-23,\n",
       "        2.26233099e-23, 3.35535389e-26],\n",
       "       [2.96530533e-22, 9.96477704e-01, 2.64985454e-03, 1.01881851e-16,\n",
       "        3.44645957e-23, 6.32006147e-23, 3.17127160e-23, 1.19220562e-18,\n",
       "        8.72441014e-04, 8.83820338e-16],\n",
       "       [2.21248300e-14, 3.80060471e-33, 3.05956774e-27, 9.99768671e-01,\n",
       "        2.31328887e-04, 1.42042488e-17, 2.31625631e-32, 5.64115053e-28,\n",
       "        1.04707325e-26, 3.95127831e-31],\n",
       "       [1.58425050e-10, 5.00196535e-20, 1.92553214e-28, 3.52414741e-27,\n",
       "        9.68130457e-02, 1.28313109e-28, 9.21086641e-19, 1.42903067e-26,\n",
       "        9.32404065e-26, 9.03186954e-01]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.exp(yhat)\n",
    "q = q/np.expand_dims(np.sum(q, axis = 1), axis = 1)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232af79b",
   "metadata": {},
   "source": [
    "Show that they are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53e29de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af617d",
   "metadata": {},
   "source": [
    "Compute $\\tilde {c}(n)$ (can be tricky):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda62e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), array([4, 2, 8, 3]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tilde = np.where(p) # np.where is like a condition.\n",
    "c_tilde # Numbers in each array represent an index that starts at 0. (1st image is 4th class, 3rd image is the 8th class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec129c",
   "metadata": {},
   "source": [
    "Compute the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34c2010d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(183.82401760653408)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hs = -np.log(q[c_tilde])\n",
    "L = sum(Hs)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6cc1b",
   "metadata": {},
   "source": [
    "# Proof This is Equivalent to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d666a",
   "metadata": {},
   "source": [
    "Create loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1de02c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = torch.nn.CrossEntropyLoss(reduction = 'sum') # reduction parameter sums across images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917ff8f",
   "metadata": {},
   "source": [
    "Evaluate on data above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f27c1e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(183.8240, dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(torch.tensor(yhat), torch.tensor(p, dtype = torch.float))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
